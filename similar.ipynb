{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/teng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.018*\"model\" + 0.008*\"time\" + 0.008*\"variabl\" + 0.007*\"estim\" + 0.006*\"paramet\" + 0.006*\"data\" + 0.005*\"chang\" + 0.005*\"risk\" + 0.005*\"valu\" + 0.005*\"select\"\n",
      "Topic: 1 \n",
      "Words: 0.011*\"model\" + 0.008*\"data\" + 0.007*\"estim\" + 0.006*\"time\" + 0.005*\"figur\" + 0.005*\"price\" + 0.005*\"market\" + 0.005*\"risk\" + 0.004*\"paramet\" + 0.004*\"return\"\n",
      "Topic: 2 \n",
      "Words: 0.012*\"model\" + 0.007*\"result\" + 0.006*\"data\" + 0.005*\"time\" + 0.005*\"estim\" + 0.005*\"paramet\" + 0.005*\"risk\" + 0.005*\"market\" + 0.005*\"cluster\" + 0.004*\"valu\"\n",
      "Topic: 3 \n",
      "Words: 0.010*\"model\" + 0.009*\"cluster\" + 0.007*\"estim\" + 0.007*\"price\" + 0.006*\"data\" + 0.006*\"variabl\" + 0.006*\"time\" + 0.005*\"return\" + 0.005*\"market\" + 0.005*\"result\"\n",
      "Topic: 4 \n",
      "Words: 0.012*\"model\" + 0.009*\"cluster\" + 0.009*\"data\" + 0.007*\"power\" + 0.007*\"estim\" + 0.007*\"wind\" + 0.007*\"price\" + 0.007*\"factor\" + 0.006*\"figur\" + 0.006*\"market\"\n",
      "Topic: 5 \n",
      "Words: 0.016*\"model\" + 0.009*\"chang\" + 0.009*\"paramet\" + 0.008*\"valu\" + 0.007*\"data\" + 0.007*\"method\" + 0.006*\"time\" + 0.006*\"penal\" + 0.005*\"figur\" + 0.005*\"function\"\n",
      "Topic: 6 \n",
      "Words: 0.014*\"model\" + 0.009*\"cluster\" + 0.008*\"market\" + 0.008*\"estim\" + 0.007*\"price\" + 0.007*\"data\" + 0.006*\"risk\" + 0.006*\"time\" + 0.005*\"result\" + 0.005*\"variabl\"\n",
      "Topic: 7 \n",
      "Words: 0.008*\"model\" + 0.007*\"market\" + 0.007*\"time\" + 0.006*\"data\" + 0.006*\"estim\" + 0.005*\"variabl\" + 0.005*\"figur\" + 0.005*\"risk\" + 0.004*\"valu\" + 0.004*\"result\"\n",
      "Topic: 8 \n",
      "Words: 0.014*\"model\" + 0.007*\"market\" + 0.006*\"time\" + 0.006*\"variabl\" + 0.006*\"data\" + 0.005*\"result\" + 0.005*\"network\" + 0.005*\"risk\" + 0.005*\"base\" + 0.005*\"estim\"\n",
      "Topic: 9 \n",
      "Words: 0.015*\"model\" + 0.009*\"market\" + 0.007*\"data\" + 0.007*\"price\" + 0.007*\"risk\" + 0.006*\"time\" + 0.006*\"base\" + 0.006*\"perform\" + 0.006*\"result\" + 0.005*\"estim\"\n",
      "Topic: 3, Probability: 0.34977421164512634\n",
      "Topic: 6, Probability: 0.6501711010932922\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import ssl\n",
    "import PyPDF2\n",
    "import pandas as df\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token)) \n",
    "    return result\n",
    "\n",
    "files_folder = f'./full_text/cleaned/cleaned_text/'\n",
    "\n",
    "results = defaultdict(list)\n",
    "for file in Path(files_folder).glob('**/*.txt'):\n",
    "    with open(file, \"r\") as file_open:\n",
    "        results[\"file_name\"].append(file.name)\n",
    "        results[\"text\"].append(file_open.read())\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "data_text = df[['text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "\n",
    "processed_docs = documents['text'].map(preprocess)\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "\n",
    "# Folder where the new document is\n",
    "folder_path = './similar_text/'\n",
    "\n",
    "# Name of the PDF file\n",
    "pdf_file_name = '20230415 BW Bingling Wang DISS.pdf' \n",
    "\n",
    "# Read in the PDF file\n",
    "with open(os.path.join(folder_path, pdf_file_name), 'rb') as file:\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "    new_doc = ''\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        new_doc += pdf_reader.pages[page_num].extract_text()\n",
    "\n",
    "# Preprocess the new document\n",
    "new_doc_preprocessed = preprocess(new_doc)\n",
    "\n",
    "# Transform the document into bag of words format\n",
    "new_doc_bow = dictionary.doc2bow(new_doc_preprocessed)\n",
    "\n",
    "# Get the topics\n",
    "topics = lda_model.get_document_topics(new_doc_bow)\n",
    "\n",
    "# Print the topics\n",
    "for topic, probability in topics:\n",
    "    print(f'Topic: {topic}, Probability: {probability}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
